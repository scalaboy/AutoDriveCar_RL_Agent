PART A 
æ¨¡ä»¿å­¦ä¹  Imitating learing 

1ã€Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst
Bansal, Mayank, Alex Krizhevsky, and Abhijit Ogale. "Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst." arXiv preprint arXiv:1812.03079 
(2018).

ç®€ä»‹ï¼šè¿™ç¯‡æ–‡ç« æ˜¯å¼ºåŒ–å­¦ä¹ åšè‡ªåŠ¨é©¾é©¶çš„ç»å…¸ä¹‹ä½œã€‚å¼•ç”¨æ¬¡æ•°å·²ç»å¿«300äº†ã€‚
æ ¸å¿ƒæ€æƒ³æ˜¯ åœ¨3000ä¸‡ï¼ˆ60å¤©ï¼‰é©¾é©¶è®°å½•æ•°æ®ä¸­ï¼Œé€šè¿‡æ•°æ®å¢å¼ºçš„æ–¹å¼ï¼Œæ„å»ºä¸€äº›ç¢°æ’ã€å¼‚å¸¸å¤„ç†çš„æ•°æ®ï¼Œä»è€Œæå‡æ¨¡ä»¿å­¦ä¹ æ ·æœ¬çš„å¤šæ ·æ€§ã€‚

å…¶ä»–æ–¹é¢ï¼šéend2endï¼Œè¾“å…¥çš„æ˜¯ä¸€ä¸ªåŸå§‹å›¾åƒæ•°æ®å¤„ç†è¿‡çš„ä¸­é—´ç»“æœï¼Œè¾“å‡ºçš„ä¹Ÿæ˜¯ä¸€ä¸ªä¸­é—´ç»“æœï¼Œç„¶åé€šè¿‡æ§åˆ¶æ¨¡å—ç»¼åˆå¤„ç†æ“ä½œæ±½è½¦ã€‚
åœ¨å®é™…æ“ä½œä¸­æœ‰çœŸå®é©¾é©¶æµ‹è¯•ã€‚

ç¼ºç‚¹ï¼šç›®å‰æ¨¡ä»¿å­¦ä¹ åšè‡ªåŠ¨é©¾é©¶ä¸æ˜¯ä¸»æµæ–¹å¼ã€‚ä¸å¤Ÿé²æ£’ã€‚ä½†æ˜¯å†å²æ€æƒ³æ¥æºä¹‹ä¸€ã€‚å€¼å¾—å­¦ä¹ ç ”ç©¶ã€‚æ‡‚å†å²æ‰èƒ½çŸ¥æœªæ¥ã€‚


PART B model-free 

2ã€Driving in Dense Traffic with Model-Free Reinforcement Learning
Dhruv Mauria Saxena1, Sangjae Bae2, Alireza Nakhaei3, Kikuo Fujimura3, Maxim Likhachev1 ICRA2020

ä¸»è¦æ˜¯è§£å†³å¯å˜è½¦é“çŸ­ï¼Œæ‹è§’å¤„æ€ä¹ˆå­¦ç­–ç•¥çš„é—®é¢˜
![image](https://user-images.githubusercontent.com/10848033/115192590-98866480-a11d-11eb-9173-ad7d2afab194.png)


PART C model-base

We conducted experiments under
three different scenarios, straight lane, 90 turning, and
circular roundabout.


PART D Inverse Reinforcement Learning

1 Adversarial Inverse Reinforcement Learning with Self-attention Dynamics Model
Jiankai Sun,  Lantao Yu,  Pinqian Dong, Bo Lu, Bolei Zhou
The Chinese Univsersity of Hong Kong, Stanford Univsersity,
Huazhong University of Science and Technology

è¿™ç¯‡æ–‡ç« æ˜¯é’ˆå¯¹æœºå™¨äººç®—æ³•åšçš„ä¼˜åŒ–ï¼Œä½†å¾ˆæœ‰å€Ÿé‰´æ„ä¹‰ã€‚ä»–æœ¬è´¨æ˜¯æ„å»ºäº†ä¸€ä¸ªå¯¹æŠ—é€†å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œèƒ½å¤Ÿæ›´åŠ æœ‰æ•ˆçš„æ‹Ÿåˆä¸“å®¶è¡Œä¸ºã€‚å¤§å®¶çŸ¥é“ï¼Œé€†å¼ºåŒ–å­¦ä¹ æ˜¯æ ¹æ®ä¸“å®¶è¡Œä¸ºï¼Œæ¥å­¦ä¹ ä¸€ä¸ªæ¿€åŠ±å‡½æ•°ï¼Œ
ç”¨æ¥æ„å»ºæ¨¡å‹æ¥æ‹Ÿåˆä¸“å®¶çš„è¡Œä¸ºï¼Œä½†è¿™ä¸ªè¿‡ç¨‹çš„ç›‘ç£ä¿¡å·æ˜¯æ¯”è¾ƒå¼±çš„ï¼Œé€šè¿‡å¯¹æŠ—é€†å¼ºåŒ–å­¦ä¹ ï¼Œæ¥æœ‰æ•ˆå¢å¼ºå­¦ä¹ åˆ°çš„æ¨¡å‹çš„ç²¾ç¡®æ€§ã€‚

2ã€Incorporating Multi-Context Into the Traversability Map for Urban Autonomous Driving Using Deep Inverse Reinforcement Learning
David Hyunchul Shim
School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea
IEEE Robotics and Automation Letters ( Volume: 6, Issue: 2, April 2021) 

è¿™ç¯‡æ–‡ç« æ˜¯éŸ©å›½å°å“¥çš„æ–‡ç« ã€‚ä¹Ÿå¾ˆæœ‰æ„æ€ã€‚ä»–æ˜¯æŠŠæ•´ä¸ªç¯å¢ƒï¼ˆåŒ…æ‹¬è¡Œäºº è½¦ é“è·¯ï¼‰çš„ä¸Šä¸‹æ–‡åŠ¨æ€ä¿¡æ¯ç³…åˆè¿›æ•´ä¸ªnetï¼Œç„¶åé¢„æµ‹ä¸€ä¸ªå¯é©¾é©¶å›¾ã€‚ä»è€Œæå‡é©¾é©¶è¡Œä¸ºçš„ç²¾åº¦ã€‚å®éªŒè¡¨æ˜æ•´ä¸ªé©¾é©¶è½¨è¿¹è·Ÿäººç±»é©¾é©¶è½¨è¿¹å¾ˆæ¥è¿‘ã€‚è¿™ä¸ªæ–¹æ³•å·²ç»åœ¨å®é™…æµ‹è¯•è½¦ä¸Šç”¨äº†

PART M  Multi-agent self-driving

1ã€P. Palanisamy, â€œMulti-agent connected autonomous driving using deep
reinforcement learning,â€ in 2020 International Joint Conference on
Neural Networks (IJCNN). IEEE, 2020

2ã€S. Bhalla, S. Ganapathi Subramanian, and M. Crowley, â€œDeep multi
agent reinforcement learning for autonomous driving,â€ in Advances in
Artificial Intelligence, C. Goutte and X. Zhu, Eds. Cham: Springer
International Publishing, 2020,

3ã€A. Wachi, â€œFailure-scenario maker for rule-based agent using multiagent
adversarial reinforcement learning and its application to autonomous
driving,â€ arXiv preprint arXiv:1903.10654, 2019.

4ã€C. Yu, X. Wang, X. Xu, M. Zhang, H. Ge, J. Ren, L. Sun, B. Chen, and
G. Tan, â€œDistributed multiagent coordinated learning for autonomous
driving in highways based on dynamic coordination graphs,â€ IEEE
Transactions on Intelligent Transportation Systems, vol. 21, no. 2, pp.
735â€“748, 2020.




